# Выполнение задания 14
## Пункт №1. Формулировка задания из методички
Задание 14: Neural Style Transfer

Задача: реализовать Neural Style Transfer для применения стиля одного
изображения к другому.

Требования:

Использовать VGG19 как feature extractor
Gram matrix для стиля
Content loss и Style loss

Оптимизация изображения через gradient descent

## Пункт №2. Формирование промпта
«Напиши на Python реализацию Neural Style Transfer в TensorFlow 2 с использованием предобученной VGG19 как feature extractor.

Требования:

– класс NeuralStyleTransfer с методами: gram_matrix, content_loss, style_loss, total_loss, transfer_style;

– Gram‑матрица считается через tf.linalg.einsum;

– контент‑ и стиль‑лоссы объединяются в total_loss с весами content_weight и style_weight;

– в transfer_style нужно оптимизировать сгенерированное изображение с помощью Adam, считать количество эпох как параметр, и на каждой эпохе логировать значения лоссов;

– добавить функции загрузки и показа изображений (контент и стиль), а также построение графиков total/content/style loss по эпохам с помощью matplotlib;

– код должен быть полностью рабочим в Google Colab при наличии файлов content.jpg и style.jpg в /content.»

## Пункт №3. Алгоритм работы НС по блокам
Алгоритм можно разложить на несколько логических блоков: от подготовки данных до оптимизации изображения.

Блок 1. Подготовка и загрузка моделей

Загрузить предобученную VGG19 без полносвязной «головы» (include_top=False, веса ImageNet) и заморозить её параметры.

Построить модель, которая по входному изображению возвращает набор выходов с выбранных слоёв (content‑слои и style‑слои).

Блок 2. Подготовка изображений

Загрузить контент‑ и стиль‑изображения, изменить их размер до нужного (масштабирование с сохранением пропорций).

Преобразовать в тензоры float32 и выполнить такую же предобработку, которую ожидает VGG19 (центрирование/нормализация каналов).

Блок 3. Выделение признаков

Прогнать стиль‑изображение через feature‑extractor и получить:

карты признаков для style‑слоёв;

карты признаков для content‑слоёв (если нужно).

Аналогично определить функции, которые для любого изображения возвращают:

Gram‑матрицы для style‑слоёв (характеризуют текстуру/стиль);

обычные feature‑maps для content‑слоёв (содержимое сцены).

Блок 4. Функции потерь

Content loss: среднеквадратичное отклонение между контент‑признаками исходного контента и сгенерированного изображения.

Style loss: среднеквадратичное отклонение между Gram‑матрицами стиля и Gram‑матрицами сгенерированного изображения по всем style‑слоям.

Total loss: взвешенная сумма content loss и style loss с коэффициентами content_weight и style_weight.

Блок 5. Инициализация оптимизируемого изображения
Взять начальным приближением само контент‑изображение (или белый шум) и объявить его обучаемой переменной.

Создать оптимизатор (например, Adam) с заданным learning rate.

Блок 6. Цикл оптимизации (обучение)

Для каждой эпохи:

Включить вычисление градиентов (градиентный контекст).

Посчитать total/content/style loss для текущего generated_image.

Найти градиент total loss по пикселям generated_image.

Обновить generated_image шагом оптимизатора.

Сохранить значения лоссов в историю и периодически выводить их в консоль.

Блок 7. Визуализация и сохранение результатов

После завершения цикла денормализовать и отобразить финальное изображение (перенесённый стиль).

Построить графики total, content и style loss по эпохам, чтобы проанализировать сходимость.

Сохранить полученное изображение (и при желании — графики) на диск.
